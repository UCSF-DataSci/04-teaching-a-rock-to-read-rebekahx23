{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466204de",
   "metadata": {},
   "source": [
    "# The Lazy Book Report\n",
    "\n",
    "Your professor has assigned a book report on \"The Red-Headed League\" by Arthur Conan Doyle. \n",
    "\n",
    "You haven't read the book. And out of stubbornness, you won't.\n",
    "\n",
    "But you *have* learned NLP. Let's use it to answer the professor's questions without reading.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's fetch the text from Project Gutenberg and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a46884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story loaded: 4000 words in 3 sections\n",
      "Section sizes: [1333, 1333, 1334]\n"
     ]
    }
   ],
   "source": [
    "# Fetch and prepare text - RUN THIS CELL FIRST\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Python-urllib'})\n",
    "with urllib.request.urlopen(req, timeout=30) as resp:\n",
    "    text = resp.read().decode('utf-8')\n",
    "\n",
    "# Strip Gutenberg boilerplate\n",
    "text = text.split('*** START OF')[1].split('***')[1]\n",
    "text = text.split('*** END OF')[0]\n",
    "\n",
    "# Extract \"The Red-Headed League\" story (it's the second story in the collection)\n",
    "matches = list(re.finditer(r'THE RED-HEADED LEAGUE', text, re.IGNORECASE))\n",
    "story_start = matches[1].end()\n",
    "story_text = text[story_start:]\n",
    "story_end = re.search(r'\\n\\s*III\\.\\s*\\n', story_text)\n",
    "story_text = story_text[:story_end.start()] if story_end else story_text\n",
    "\n",
    "# Split into 3 sections by word count\n",
    "words = story_text.split()[:4000]\n",
    "section_size = len(words) // 3\n",
    "sections = [\n",
    "    ' '.join(words[:section_size]),\n",
    "    ' '.join(words[section_size:2*section_size]),\n",
    "    ' '.join(words[2*section_size:])\n",
    "]\n",
    "\n",
    "print(f\"Story loaded: {len(words)} words in {len(sections)} sections\")\n",
    "print(f\"Section sizes: {[len(s.split()) for s in sections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e757a",
   "metadata": {},
   "source": [
    "## Professor's Questions\n",
    "\n",
    "Your professor wants you to answer 5 questions about the story. Let's use NLP to find the answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1: Writing Style\n",
    "\n",
    "> \"This text is from the 1890s. What makes it different from modern writing?\"\n",
    "\n",
    "**NLP Method:** Use preprocessing to compute text statistics. Tokenize the text and calculate:\n",
    "- Vocabulary richness (unique words / total words)\n",
    "- Average sentence length\n",
    "- Average word length\n",
    "\n",
    "**Hint:** Formal, literary writing typically shows higher vocabulary richness and longer sentences than modern casual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_richness': 0.09966732367633409, 'avg_sentence_length': 14.550682852807284, 'avg_word_length': 4.191930252688004}\n"
     ]
    }
   ],
   "source": [
    "# Your code here: compute text statistics\n",
    "# You'll need: import string, import re\n",
    "# - Tokenize: remove punctuation, lowercase\n",
    "# - Sentences: split on sentence-ending punctuation\n",
    "# Calculate vocab_richness, avg_sentence_length, avg_word_length\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "def compute_text_statistics(text):\n",
    "    # Tokenize\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    tokens = text.translate(translator).lower().split()\n",
    "    \n",
    "    # Unique words\n",
    "    unique_words = set(tokens)\n",
    "    vocab_richness = len(unique_words) / len(tokens) if tokens else 0\n",
    "    \n",
    "    # Sentences\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    avg_sentence_length = len(tokens) / len(sentences) if sentences else 0\n",
    "    \n",
    "    # Average word length\n",
    "    total_word_length = sum(len(word) for word in tokens)\n",
    "    avg_word_length = total_word_length / len(tokens) if tokens else 0\n",
    "    \n",
    "    return {\n",
    "        'vocab_richness': vocab_richness,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length\n",
    "    }\n",
    "\n",
    "print(compute_text_statistics(story_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Main Characters\n",
    "\n",
    "> \"Who are the main characters in this story?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract PERSON entities.\n",
    "\n",
    "**Hint:** Use spaCy's `en_core_web_sm` model. Process the text and filter entities where `ent.label_ == 'PERSON'`. Count how often each name appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a59da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 262 unique PERSON entities.\n"
     ]
    }
   ],
   "source": [
    "# Your code here: extract PERSON entities using spaCy NER\n",
    "# You'll need: import spacy, nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/characters.txt\", \"w\") as f:\n",
    "#     for name in your_character_list:\n",
    "#         f.write(f\"{name}\\n\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(story_text)\n",
    "persons = set(ent.text for ent in doc.ents if ent.label_ == \"PERSON\")\n",
    "\n",
    "with open(\"output/characters.txt\", \"w\") as f:\n",
    "    for name in persons:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "print(f\"Extracted {len(persons)} unique PERSON entities.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732e661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Story Locations\n",
    "\n",
    "> \"Where does the story take place?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract location entities (GPE and LOC).\n",
    "\n",
    "**Hint:** Filter entities where `ent.label_` is 'GPE' (geopolitical entity) or 'LOC' (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f8f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 116 unique GPE and LOC entities.\n"
     ]
    }
   ],
   "source": [
    "# Your code here: extract GPE and LOC entities using spaCy NER\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/locations.txt\", \"w\") as f:\n",
    "#     for place in your_locations_list:\n",
    "#         f.write(f\"{place}\\n\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(story_text)\n",
    "locations = set(ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"})\n",
    "with open(\"output/locations.txt\", \"w\") as f:\n",
    "    for place in locations:\n",
    "        f.write(f\"{place}\\n\")\n",
    "print(f\"Extracted {len(locations)} unique GPE and LOC entities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b228d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Wilson's Business\n",
    "\n",
    "> \"What is Wilson's business?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's business.\n",
    "\n",
    "**Hint:** Create a TF-IDF vectorizer, fit it on the 3 sections, then transform your query using the same vectorizer (`.transform()`, not `.fit_transform()` - you want to use the vocabulary learned from the sections). Find which section has the highest cosine similarity and read it to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Most relevant section regarding Wilson's business is section 2 has been saved to output/business.txt\n"
     ]
    }
   ],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "# You'll need: from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#              from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/business.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's business is: ...\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sections)\n",
    "\n",
    "question = \"What is Wilson's business?\"\n",
    "question_vec = vectorizer.transform([question])\n",
    "similarities = cosine_similarity(question_vec, X).flatten()\n",
    "most_relevant_section = sections[similarities.argmax()]\n",
    "\n",
    "with open(\"output/business.txt\", \"w\") as f:\n",
    "    f.write(\"Wilson's business is: He runs a small pawnbroker’s shop at Coburg Square near the City, which earns him just enough to make a living and employs one assistant, Vincent Spaulding.\\n\")\n",
    "\n",
    "print(f\"The Most relevant section regarding Wilson's business is section {similarities.argmax()+1} has been saved to output/business.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85452bf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Wilson's Work Routine\n",
    "\n",
    "> \"What is Wilson's daily work routine for the League?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's work routine.\n",
    "\n",
    "**Hint:** Similar to Question 4 - use TF-IDF to find the section that best matches your query about work routine. The answer includes what Wilson had to do and what eventually happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddea14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Most relevant section regarding Wilson's work routine is section 3 has been saved to output/routine.txt\n"
     ]
    }
   ],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/routine.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's work routine: ...\\n\")\n",
    "#     f.write(\"What happened: ...\\n\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sections)\n",
    "question = \"What is Wilson's daily work routine for the League?\"\n",
    "question_vec = vectorizer.transform([question])\n",
    "similarities = cosine_similarity(question_vec, X).flatten()\n",
    "most_relevant_section = sections[similarities.argmax()]\n",
    "with open(\"output/routine.txt\", \"w\") as f:\n",
    "    f.write(\"Wilson's work routine: Wilson reports to the League office every morning from 10 a.m. to 2 p.m., where he must remain inside the room the entire time and copy articles from the Encyclopaedia Britannica by hand. He is not allowed to leave for any reason and is paid £4 per week for this work.\\n\")\n",
    "    f.write(\"What happened: After eight weeks of consistent work and payment, Wilson arrived one morning to find the office locked and a notice on the door stating that the Red-Headed League had been dissolved.\\n\")\n",
    "    print(f\"The Most relevant section regarding Wilson's work routine is section {similarities.argmax()+1} has been saved to output/routine.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
